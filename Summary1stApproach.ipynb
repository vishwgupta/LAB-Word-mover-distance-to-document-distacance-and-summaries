{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.data\n",
    "import string\n",
    "from pyemd import emd\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "import numpy.linalg as la\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import scipy.spatial as spt\n",
    "from scipy.spatial import distance\n",
    "from pyemd import emd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from numpy import dot, zeros, dtype, array, sqrt,double,array,sqrt, sum as np_sum\n",
    "import time\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from scipy.interpolate import Rbf\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    numberOfFiles = len(r)\n",
    "    return r\n",
    "\n",
    "path = os.path.join(os.path.expanduser('.'),'summaries','docs')\n",
    "fnames = list_files(path)\n",
    "numberOfFiles = len(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### FirstLineAllPara....processedFirstLineAllPara,,,,, \n",
    "#processedAllList,,,,,rawAllList....\n",
    "#LastSentEachFile...processedLastSentEachFile\n",
    "\n",
    "paraFile = []\n",
    "processedAllList= []\n",
    "rawAllList =[]\n",
    "firstSentence = []\n",
    "lastSentence = []\n",
    "FirstLineAllPara = []\n",
    "LastPara =[]\n",
    "LastSentEachFile = []\n",
    "for name in (fnames):\n",
    "    with open(name, 'r') as f:\n",
    "        data = f.read().lower()\n",
    "        paragraphs = data.split(\"\\n\\n\")\n",
    "        paragraphs[:] = (value for value in paragraphs if value != '\\t')\n",
    "        LastPara.append(paragraphs[-1])\n",
    "    paraFile.extend(paragraphs)\n",
    "for i in range(len(LastPara)):\n",
    "    LasteachPara =[]\n",
    "    for j in (LastPara[i].strip().split('. ')):\n",
    "        LasteachPara.append(j)\n",
    "    LastSentEachFile.append(LasteachPara[-1])\n",
    "processedLastSentEachFile = []\n",
    "for i in range(len(LastSentEachFile)):\n",
    "    line = LastSentEachFile[i]\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    Str2 = ('\\n-----\\n'.join(sent_detector.tokenize(line.strip())))\n",
    "    Str2 = re.sub(' +', ' ', Str2)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(Str2)\n",
    "    Str1 = \" \".join(filter(lambda word: word not in stop_words, Str2.split()))\n",
    "    Str1 = re.sub(r'[^\\w\\s]','',Str1) \n",
    "    if(Str1 != ''):\n",
    "        processedLastSentEachFile.append(Str1)\n",
    "\n",
    "for para in range(len(paraFile)):\n",
    "    eachPara = paraFile[para]\n",
    "    for firstLine in (eachPara.strip().split('. ')):\n",
    "        FirstLineAllPara.append(firstLine)\n",
    "        break     \n",
    "processedFirstLineAllPara = []\n",
    "for i in range(len(FirstLineAllPara)):\n",
    "    line = FirstLineAllPara[i]\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    Str2 = ('\\n-----\\n'.join(sent_detector.tokenize(line.strip())))\n",
    "    Str2 = re.sub(' +', ' ', Str2)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(Str2)\n",
    "    Str1 = \" \".join(filter(lambda word: word not in stop_words, Str2.split()))\n",
    "    Str1 = re.sub(r'[^\\w\\s]','',Str1) \n",
    "    if(Str1 != ''):\n",
    "        processedFirstLineAllPara.append(Str1)\n",
    "processedList= []\n",
    "rawList = []   \n",
    "RawList = []\n",
    "sents = None\n",
    "text1 = None\n",
    "text1 = paraFile\n",
    "text =[]\n",
    "for elem in text1:\n",
    "    text.extend(elem.strip().split('. '))  \n",
    "\n",
    "sent = [el.replace('\\n', '') for el in text]\n",
    "if(sents ==None):\n",
    "    sents = sent\n",
    "else:\n",
    "    sents = sents+sent\n",
    "rawList = sents\n",
    "for i in range(len(rawList)):\n",
    "    l = rawList[i]\n",
    "    if(l != ''):\n",
    "        RawList.append(l)\n",
    "rawAllList = rawAllList+ RawList\n",
    "processedAllList =[]\n",
    "for i in range(len(rawAllList)):\n",
    "    line = rawAllList[i]\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    Str2 = ('\\n-----\\n'.join(sent_detector.tokenize(line.strip())))\n",
    "    Str2 = re.sub(' +', ' ', Str2)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(Str2)\n",
    "    Str1 = \" \".join(filter(lambda word: word not in stop_words, Str2.split()))\n",
    "    Str1 = re.sub(r'[^\\w\\s]','',Str1) \n",
    "    if(Str1 != ''):\n",
    "        processedList.append(Str1)\n",
    "    processedAllList.append(Str1)        \n",
    "#lastSentence = processedAllList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### FirstLineAllPara....processedFirstLineAllPara,,,,, \n",
    "#processedAllList,,,,,rawAllList....\n",
    "#LastSentEachFile...processedLastSentEachFile\n",
    "print(len(processedAllList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vec = None\n",
    "array = None\n",
    "global words_1\n",
    "global words_2\n",
    "global words_1_set\n",
    "global words_2_set\n",
    "removed_words = ['sdkls']\n",
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with open(\"data/embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FileOpen(line1,line2):  \n",
    "    global woduplicates\n",
    "    global words_1\n",
    "    woduplicates = None\n",
    "    words_1 = line1.split()\n",
    "    words_1 = [token for token in words_1 if token in vocab_dict.keys()]\n",
    "    global words_2\n",
    "    words_2 = line2.split()\n",
    "    words_2 = [token for token in words_2 if token in vocab_dict.keys()] \n",
    "    woduplicates = set(words_1 + words_2)\n",
    "    return woduplicates\n",
    "def Dictionaty2Files(content, woduplicates):\n",
    "    doc_len = len(content)\n",
    "    d = []\n",
    "    for i, t in enumerate(woduplicates):\n",
    "        d.append(content.count(t) / float(doc_len))\n",
    "    return d \n",
    "def distance_matrix():\n",
    "    vocab_len = len(woduplicates)\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=float)\n",
    "    for i, t1 in enumerate(woduplicates):\n",
    "        for j, t2 in enumerate(woduplicates):\n",
    "            distance_matrix[i][j] = sqrt(np_sum((W[[vocab_dict[t1]]] - W[[vocab_dict[t2]]])**2))\n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allWords = [' '.join(processedAllList)]\n",
    "mostCommonWords = Counter(allWords[0].split()).most_common(5)\n",
    "reslist = [x[0] for x in mostCommonWords]\n",
    "res_list = [token for token in reslist if token in vocab_dict.keys()]\n",
    "print(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###GENERAL IDEA SENTENCES BASED ON MOST FREQUENT WORDS###\n",
    "combo =[]\n",
    "for w in range(len(res_list)):\n",
    "    word1 = res_list[w]\n",
    "    global woduplicates\n",
    "    global words_1\n",
    "    WMD_DICT = dict()\n",
    "    selectedAllSentences =[0,0]\n",
    "    M = len(processedAllList)\n",
    "    distanceMatrix = np.zeros((1,M))\n",
    "    Line = word1\n",
    "    for i in range(M):\n",
    "        Line1 = processedAllList[i]  \n",
    "        woduplicates = FileOpen(Line1,Line)\n",
    "        if(len(words_2)>0 and len(words_1)>0):\n",
    "            d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "            d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "            distance = distance_matrix()\n",
    "            distanceWMD = emd(d1,d2,distance)\n",
    "            WMD_DICT.update({(rawAllList[i],distanceWMD )})\n",
    "\n",
    "    sorted_K_WMD = (sorted(WMD_DICT.items(), key=lambda x:x[1]))\n",
    "    k = 5\n",
    "    sorted_K_WMD_array = np.asarray(sorted_K_WMD[0:k])\n",
    "    \n",
    "    combo.extend(list(sorted_K_WMD_array[:,0]))\n",
    "    \n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(combo)\n",
    "c_ = (c.most_common(5))\n",
    "SentFreqWords =res_list = [x[0] for x in c_] ####IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###GENERAL IDEA SENENCES FROM LAST SENTENCE OF EACH PARAGRAPH\n",
    "\n",
    "#FirstLineAllPara....processedFirstLineAllPara,,,,, \n",
    "#processedAllList,,,,,rawAllList....\n",
    "#LastSentEachFile...processedLastSentEachFile\n",
    "leng = len(processedLastSentEachFile)\n",
    "M =len(processedAllList)\n",
    "distanceMatrix = np.zeros((leng,M))\n",
    "for i in range(leng):\n",
    "    Line = processedLastSentEachFile[i]\n",
    "    for j in range(M):\n",
    "        Line1 = processedAllList[j]  \n",
    "        woduplicates = FileOpen(Line,Line1)\n",
    "\n",
    "        if(len(words_2)>0 and len(words_1)>0):\n",
    "            d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "            d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "            distance = distance_matrix()\n",
    "            distanceWMD = emd(d1,d2,distance)\n",
    "            distanceMatrix[i][j] = distanceWMD\n",
    "\n",
    "SentLastLine =[]\n",
    "for row in range(leng):\n",
    "    minRow = min(i for i in distanceMatrix[row] if i > 0)\n",
    "    indesOfValuemin = np.where(distanceMatrix == minRow)\n",
    "    iandj = ([x[0] for x in indesOfValuemin])\n",
    "    i = iandj[0]\n",
    "    j = iandj[1]\n",
    "    SentLastLine.append(rawAllList[j])\n",
    "    \n",
    "    min2Row = min(i for i in distanceMatrix[row] if i > minRow)\n",
    "    indesOfValuemin = np.where(distanceMatrix == min2Row)\n",
    "    iandj = ([x[0] for x in indesOfValuemin])\n",
    "    i = iandj[0]\n",
    "    j = iandj[1]\n",
    "    SentLastLine.append(rawAllList[j])   \n",
    "print(SentLastLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GenralIdea = SentLastLine+processedFirstLineAllPara+SentFreqWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(processedAllList))\n",
    "print(len(GenralIdea))\n",
    "print(len(GenralIdea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SimilarityPara(para):\n",
    "    processedEachSentPara=[]\n",
    "    UnprocessedPara = []\n",
    "    eachPara = para\n",
    "    for firstLine in (eachPara):\n",
    "        line = firstLine\n",
    "        UnprocessedPara.append(line)\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        Str2 = ('\\n-----\\n'.join(sent_detector.tokenize(line.strip())))\n",
    "        Str2 = re.sub(' +', ' ', Str2)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = nltk.word_tokenize(Str2)\n",
    "        Str1 = \" \".join(filter(lambda word: word not in stop_words, Str2.split()))\n",
    "        Str1 = re.sub(r'[^\\w\\s]','',Str1) \n",
    "        if(Str1 != ''):\n",
    "            processedEachSentPara.append(Str1)\n",
    "    leng = len(processedEachSentPara)\n",
    "    distanceMatrix = np.zeros((leng,leng))\n",
    "    for i in range(leng):\n",
    "        Line = processedEachSentPara[i]\n",
    "        for j in range(leng):\n",
    "            Line1 = processedEachSentPara[j]  \n",
    "            woduplicates = FileOpen(Line,Line1)\n",
    "\n",
    "            if(len(words_2)>0 and len(words_1)>0):\n",
    "                d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "                d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "                distance = distance_matrix()\n",
    "                distanceWMD = emd(d1,d2,distance)\n",
    "                distanceMatrix[i][j] = distanceWMD\n",
    "                distanceMatrix[j][i] = distanceMatrix[i][j]\n",
    "    SentFirstLine = []\n",
    "    for row in range(leng):\n",
    "        minRow = min(i for i in distanceMatrix[row] if i > 0)\n",
    "        indesOfValuemin = np.where(distanceMatrix == minRow)\n",
    "        iandj = ([x[0] for x in indesOfValuemin])\n",
    "        i = iandj[0]\n",
    "        j = iandj[1]\n",
    "        SentFirstLine.append(UnprocessedPara[j])\n",
    "    GI = (set([i for i in SentFirstLine if SentFirstLine.count(i)>1]))\n",
    "    return GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Summary = SimilarityPara(GenralIdea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Summary)\n",
    "print(len(Summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
