{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyemd import emd\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "import numpy.linalg as la\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import scipy.spatial as spt\n",
    "from scipy.spatial import distance\n",
    "from pyemd import emd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import itertools\n",
    "import collections\n",
    "from numpy import dot, zeros, dtype, array, sqrt,double,array,sqrt, sum as np_sum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vec = None\n",
    "array = None\n",
    "global words_1\n",
    "global words_2\n",
    "global words_1_set\n",
    "global words_2_set\n",
    "global File\n",
    "removed_words = ['sdkls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with open(\"data/embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FileOpen(file1,file2):  \n",
    "    global woduplicates\n",
    "    woduplicates = None\n",
    "    with open(file1) as f:\n",
    "        global words_1\n",
    "        words_1 = f.read().split()\n",
    "        words_1 = [token for token in words_1 if token in vocab_dict.keys()]\n",
    "\n",
    "    with open(file2) as f:\n",
    "        global words_2\n",
    "        words_2 = f.read().split()\n",
    "        words_2 = [token for token in words_2 if token in vocab_dict.keys()]\n",
    "    \n",
    "    woduplicates = set(words_1 + words_2)\n",
    "    \n",
    "    return woduplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def centroidOfFile(words):\n",
    "    fileWords = set(words)\n",
    "    numberWords = len(fileWords)\n",
    "    VecMatrixWords =np.zeros((numberWords, 300))\n",
    "    centroid =np.zeros((1, 300))\n",
    "    for j,t in enumerate(fileWords):\n",
    "        VecMatrixWords[j] = (W[[vocab_dict[t]]])\n",
    "    centroid =np.zeros((1, 300))\n",
    "    for j in range(300):\n",
    "        for i in range(numberWords):\n",
    "            centroid[0][j] =  centroid[0][j]+(VecMatrixWords[i][j])\n",
    "        centroid[0][j]=  centroid[0][j]/300\n",
    "    return(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Dictionaty2Files(content, woduplicates):\n",
    "    doc_len = len(content)\n",
    "    d = []\n",
    "    for i, t in enumerate(woduplicates):\n",
    "        d.append(content.count(t) / float(doc_len))\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_matrix():\n",
    "    vocab_len = len(woduplicates)\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=float)\n",
    "    for i, t1 in enumerate(woduplicates):\n",
    "        #print(t1)\n",
    "        for j, t2 in enumerate(woduplicates):\n",
    "            #print(t2)\n",
    "            distance_matrix[i][j] = sqrt(np_sum((W[[vocab_dict[t1]]] - W[[vocab_dict[t2]]])**2))\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    numberOfFiles = len(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser('.'),'data','dataWMD20NEWS','docs')\n",
    "fnames = list_files(path)\n",
    "numberOfFiles = len(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "queryDocument = input(\"Please enter query document path: \")\n",
    "k=30\n",
    "#/home/vishwani/Downloads/LAB/FINALsubmissionSVEN/data/dataWMD20NEWS/testdata/querydoc\n",
    "d_wcd = dict()\n",
    "WMD_DICT = dict()\n",
    "filename1 = queryDocument\n",
    "fnames = list_files(path)\n",
    "numberFiles = len(fnames)\n",
    "for fileName2 in fnames:\n",
    "    filename2 = fileName2\n",
    "    global woduplicates\n",
    "    woduplicates = FileOpen(filename1,filename2)\n",
    "    centroid1 = centroidOfFile(words_1)\n",
    "    centroid2 = centroidOfFile(words_2)\n",
    "    d = sqrt(np_sum((centroid1 - centroid2)**2))\n",
    "    d_wcd.update({(filename2, d)})\n",
    "dict_wcd = (sorted(d_wcd.items(), key=lambda x:x[1]))\n",
    "k_wcd = dict_wcd[0:k]\n",
    "k_wcd_array =  np.asarray(k_wcd)\n",
    "kfiles_sorted_wcd = k_wcd_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fileName2 in kfiles_sorted_wcd:\n",
    "    filename2 = fileName2\n",
    "    #print(filename2)\n",
    "    global woduplicates\n",
    "    woduplicates = FileOpen(filename1,filename2)\n",
    "    if((len(words_1)!=0)and len(words_2)!=0):\n",
    "        d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "        d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "        distance = distance_matrix()\n",
    "        distanceWMD = emd(d2,d1,distance)\n",
    "        WMD_DICT.update({(filename2,distanceWMD)})\n",
    "    else:\n",
    "        print(\"skipped\")\n",
    "        print(filename2)\n",
    "sorted_K_WMD = (sorted(WMD_DICT.items(), key=lambda x:x[1]))\n",
    "sorted_K_WMD_array = np.asarray(sorted_K_WMD[0:k])\n",
    "print(sorted_K_WMD_array)\n",
    "end = time.time\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
