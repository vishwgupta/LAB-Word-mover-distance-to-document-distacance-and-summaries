{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyemd import emd\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "import numpy.linalg as la\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import scipy.spatial as spt\n",
    "from scipy.spatial import distance\n",
    "from pyemd import emd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import itertools\n",
    "import collections\n",
    "from numpy import dot, zeros, dtype, array, sqrt,double,array,sqrt, sum as np_sum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vec = None\n",
    "array = None\n",
    "global words_1\n",
    "global words_2\n",
    "global words_1_set\n",
    "global words_2_set\n",
    "global File\n",
    "removed_words = ['sdkls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with open(\"data/embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def centroidOfFile(words):\n",
    "    fileWords = set(words)\n",
    "    numberWords = len(fileWords)\n",
    "    VecMatrixWords =np.zeros((numberWords, 300))\n",
    "    centroid =np.zeros((1, 300))\n",
    "    for j,t in enumerate(fileWords):\n",
    "        VecMatrixWords[j] = (W[[vocab_dict[t]]])\n",
    "    centroid =np.zeros((1, 300))\n",
    "    for j in range(300):\n",
    "        for i in range(numberWords):\n",
    "            centroid[0][j] =  centroid[0][j]+(VecMatrixWords[i][j])\n",
    "        centroid[0][j]=  centroid[0][j]/300\n",
    "    return(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FileOpen(file1,file2):  \n",
    "    global woduplicates\n",
    "    woduplicates = None\n",
    "    with open(file1) as f:\n",
    "        global words_1\n",
    "        global words_1_set\n",
    "        words_1 = f.read().split()\n",
    "        words_1 = [token for token in words_1 if token in vocab_dict.keys()]\n",
    "        words_1_set = set(words_1)\n",
    "    with open(file2) as f:\n",
    "        global words_2\n",
    "        global words_2_set\n",
    "        words_2 = f.read().split()\n",
    "        words_2 = [token for token in words_2 if token in vocab_dict.keys()]\n",
    "        words_2_set = set(words_2)\n",
    "    \n",
    "    woduplicates = set(words_1 + words_2)\n",
    "    return woduplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dictionaty2Files(content, woduplicates):\n",
    "    doc_len = len(content)\n",
    "    d = []\n",
    "    for i, t in enumerate(woduplicates):\n",
    "        d.append(content.count(t) / float(doc_len))\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_matrixRWMD():\n",
    "    len1 = len(words_1_set)\n",
    "    len2 = len(words_2_set)\n",
    "    T = zeros((len1, len2), dtype=float)\n",
    "    distance = zeros((len1, len2), dtype=float)\n",
    "    for i, t1 in enumerate(words_1_set):\n",
    "        for j, t2 in enumerate(words_2_set):\n",
    "            distance[i][j] = sqrt(np_sum((W[[vocab_dict[t1]]] - W[[vocab_dict[t2]]])**2))\n",
    "        minimum = np.argmin(distance[i])\n",
    "        T[i,minimum] = d1[i]*distance[i][minimum]\n",
    "    sum1 = T.sum()\n",
    "    T = zeros((len2, len1), dtype=float)\n",
    "    distance = zeros((len2, len1), dtype=float)\n",
    "    for i, t1 in enumerate(words_2_set):\n",
    "        for j, t2 in enumerate(words_1_set):\n",
    "            distance[i][j] = sqrt(np_sum((W[[vocab_dict[t1]]] - W[[vocab_dict[t2]]])**2))\n",
    "        minimum = np.argmin(distance[i])\n",
    "        T[i,minimum] = d1[j]*distance[i][minimum]\n",
    "    \n",
    "    sum2 = T.sum()\n",
    "    return max(sum1,sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_matrix():\n",
    "    vocab_len = len(woduplicates)\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=float)\n",
    "    for i, t1 in enumerate(woduplicates):\n",
    "        for j, t2 in enumerate(woduplicates):\n",
    "            distance_matrix[i][j] = sqrt(np_sum((W[[vocab_dict[t1]]] - W[[vocab_dict[t2]]])**2))\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    numberOfFiles = len(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path = os.path.join(os.path.expanduser('~'), 'Downloads','LAB', '20news-bydate-train','test')\n",
    "path = os.path.join(os.path.expanduser('.'),'data','dataWMD20NEWS','docs')\n",
    "fnames = list_files(path)\n",
    "numberOfFiles = len(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter query document path: /home/vishwani/Downloads/LAB/FINALsubmissionSVEN/data/dataWMD20NEWS/querydoc\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "queryDocument = input(\"Please enter query document path: \")\n",
    "k=30\n",
    "d_wcd = dict()\n",
    "WMD_DICT = dict()\n",
    "RWMD_DICT = dict()\n",
    "filename1 = queryDocument\n",
    "fnames = list_files(path)\n",
    "numberFiles = len(fnames)\n",
    "REMAIN = (numberFiles-k)\n",
    "for fileName2 in fnames:\n",
    "    filename2 = fileName2\n",
    "    global woduplicates\n",
    "    woduplicates = FileOpen(filename1,filename2)\n",
    "    centroid1 = centroidOfFile(words_1)\n",
    "    centroid2 = centroidOfFile(words_2)\n",
    "    d = sqrt(np_sum((centroid1 - centroid2)**2))\n",
    "    d_wcd.update({(filename2, d)})\n",
    "dict_wcd = (sorted(d_wcd.items(), key=lambda x:x[1]))\n",
    "k_wcd = dict_wcd[0:k]\n",
    "\n",
    "all_wcd = dict_wcd[0:numberFiles]\n",
    "remain_wcd = list(set(all_wcd) - set(k_wcd))\n",
    "remainFiles_wcd = remain_wcd[0:REMAIN]\n",
    "k_wcd_array =  np.asarray(k_wcd)\n",
    "kfiles_sorted_wcd = k_wcd_array[:,0]\n",
    "remain_wcd_array = np.asarray(remainFiles_wcd)\n",
    "remain_sorted_wcd = remain_wcd_array[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR K FILES CALCULATE WMD WITH filename1\n",
    "for fileName2 in kfiles_sorted_wcd:\n",
    "    filename2 = fileName2\n",
    "    global woduplicates\n",
    "    woduplicates = FileOpen(filename1,filename2)\n",
    "    if((len(words_1)!=0)and len(words_2)!=0):\n",
    "        d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "        d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "        distance = distance_matrix()\n",
    "        distanceWMD = emd(d2,d1,distance)\n",
    "        WMD_DICT.update({(filename2,distanceWMD )})\n",
    "    else:\n",
    "        print(\"skipped\")\n",
    "        print(filename2)\n",
    "sorted_K_WMD = (sorted(WMD_DICT.items(), key=lambda x:x[1]))\n",
    "sorted_K_WMD_array = np.asarray(sorted_K_WMD[0:k])\n",
    "print(sorted_K_WMD_array)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fileName2 in remain_sorted_wcd:\n",
    "    filename2 = fileName2\n",
    "    #print(filename2)\n",
    "    global woduplicates\n",
    "    woduplicates = FileOpen(filename1,filename2)\n",
    "    if((len(words_1)!=0)and len(words_2)!=0):\n",
    "        d1 = np.array(Dictionaty2Files(words_1, words_1_set), dtype=double)\n",
    "        d2 = np.array(Dictionaty2Files(words_2, words_2_set), dtype=double)\n",
    "        distanceRWMD = distance_matrixRWMD()\n",
    "        RWMD_DICT.update({(filename2,distanceRWMD)})\n",
    "    else:\n",
    "        print(\"skipped\")\n",
    "        print(filename2)\n",
    "\n",
    "sorted_remain_RWMD = (sorted(RWMD_DICT.items(), key=lambda x:x[1]))\n",
    "sorted_remain_RWMD_array = np.asarray(sorted_remain_RWMD[0:REMAIN])\n",
    "for i,j in enumerate(sorted_remain_RWMD_array[:,1]):\n",
    "    if(sorted_K_WMD_array[k-1,1]>sorted_remain_RWMD_array[i,1]):\n",
    "        filename2 = (sorted_remain_RWMD_array[i,0])\n",
    "        global woduplicates\n",
    "        woduplicates = FileOpen(filename1,filename2)\n",
    "        if((len(words_1)!=0)and len(words_2)!=0):\n",
    "            d1 = np.array(Dictionaty2Files(words_1, woduplicates), dtype=double)\n",
    "            d2 = np.array(Dictionaty2Files(words_2, woduplicates), dtype=double)\n",
    "            distance = distance_matrix()\n",
    "            distanceWMD = emd(d2,d1,distance)\n",
    "            if(float(sorted_K_WMD_array[k-1,1])>distanceWMD):\n",
    "                sorted_K_WMD_array[k-1,0] = filename2\n",
    "                sorted_K_WMD_array[k-1,1] = distanceWMD\n",
    "        else:\n",
    "            print(\"skipped\")\n",
    "            print(filename2)\n",
    "print(sorted_K_WMD_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_K_WMD_array = np.asarray(sorted_K_WMD[0:k])\n",
    "print(sorted_K_WMD_array)\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
